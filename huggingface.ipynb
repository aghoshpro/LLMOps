{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db89c71e-af6b-4975-bda5-dccfceac6b56",
   "metadata": {},
   "source": [
    "# HuggingFace (HF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc322a7f-e2f8-49fd-9de0-4be8a54d499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8543a8f-78c5-4e53-a5c0-c907383a85c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, TextStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d4fdb08-4b95-48d7-8d9a-94dca4ac267e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🟢 Hugging Face API token is SET\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True) # ensures that api keys come from the .env file if they exist there\n",
    "hugging_face_token = os.getenv('HF_TOKEN')\n",
    "\n",
    "if hugging_face_token:\n",
    "    print(\"🟢 Hugging Face API token is SET\")\n",
    "else:\n",
    "    print(\"🔴 Hugging Face API is NOT SET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd647f9c-a117-4ce0-b44f-392c8ba4944f",
   "metadata": {},
   "source": [
    "## Messages\n",
    "\n",
    "##### messages = [\n",
    "######     {\"role\": \"system\", \"content\": \"You are a helpful assistant who answers in one sentence in bold style with a suitable emoji\"},\n",
    "######     {\"role\": \"user\", \"content\": \"Who is the king of the jungle?\"}\n",
    "##### ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42cf0e6d-bb3e-4c4a-abb3-9679c2cc756e",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_01 = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant who answers in one sentence in bold style with a suitable emoji\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who is the king of the jungle?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "106f047f-0b64-4e48-9b65-2c402dd36f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_02 = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant who answers in one sentence with a suitable emoji\"},\n",
    "    {\"role\": \"user\", \"content\": \"List the top 5 tallest structures in the world\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2381fd16-d5a5-4034-a0d4-4cfc8b854b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_03 = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant who answers in markdown format with a suitable emoji\"},\n",
    "    {\"role\": \"user\", \"content\": \"List the top 5 highest mountains in the world\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab70572a-08eb-49b8-8ca5-63046435a270",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00dbd741-e631-4d44-bb99-5fe068cafb15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 13841, 110, 17888, 16, 6, 89, 40, 110, 1144, 28, 67, 2]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_IN_USE = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_IN_USE)\n",
    "\n",
    "text = \"Where your treasure is, there will your heart be also\"\n",
    "# encoded_input = tokenizer(input_text)\n",
    "# encoded_input[\"input_ids\"]\n",
    "tokens = tokenizer.encode(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "facc7958-f67b-4525-a83d-5ee82089803b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 13)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text), len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6b101cf-6494-401b-8cd4-72b111569fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>Where your treasure is, there will your heart be also</s>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72fe6535-ba6a-46c2-902c-a435814d77ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.vocab # all the tokens of model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d260c21-c85c-4fae-87a8-ae03f4d212e3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f1897d8-49ef-43cb-be03-83a8909d6f56",
   "metadata": {},
   "source": [
    "### Gated Model - Mistral Ai\n",
    "- https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c370c5d3-dc1b-42c0-a3e5-e9b513210728",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_IN_USE=\"mistralai/Mistral-7B-Instruct-v0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f904871-5754-45c6-b83c-d7231009cf8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8becfebe124e44cb918dbd8a75d74a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): MistralRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_DETAILS = AutoModelForCausalLM.from_pretrained(MODEL_IN_USE)\n",
    "MODEL_DETAILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f996ffaa-1ea7-48de-9d88-77d79f44c463",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = messages_02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bd67484-a4d1-4900-97ca-bf9184ec7e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_IN_USE, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c02ac9b7-28d7-4113-832b-706cc1e8ec05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] You are a helpful assistant who answers in one sentence with a suitable emoji\n",
      "\n",
      "List the top 5 tallest structures in the world [/INST]\n"
     ]
    }
   ],
   "source": [
    "prompt = tokenizer.apply_chat_template(messages_02, tokenize=False, add_generation_prompt=True)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fad186af-8f42-4249-87e7-d7c92c8bf661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb6798b65d124fe08333071757cde418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'system',\n",
       "    'content': 'You are a helpful assistant who answers in one sentence with a suitable emoji'},\n",
       "   {'role': 'user',\n",
       "    'content': 'List the top 5 tallest structures in the world'},\n",
       "   {'role': 'assistant',\n",
       "    'content': ' ₁. Burj Khalifa (🇦🇪) - 828m tall, Dubai, United Arab Emirates\\n₂. Shanghai Tower (🇨🇳) - 821m tall, Shanghai, China\\n₃. Makkah Royal Clock Tower (🇸🇦) - 632m tall, Mecca, Saudi Arabia\\n₄. Merdeka 118 (🇲🇾) - 639m tall, Kuala Lumpur, Malaysia (under construction)\\n⤴️ _ Five tallest structures in the world_ ⤴️\\n\\n[Note: The ranking of the fifth structure is subject to change as new constructions are completed.]'}]}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=MODEL_IN_USE)\n",
    "pipe(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12ee959b-d0f1-495e-a76a-ab90d1b634f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f776038f82b4895a3fa6097b8436441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'system',\n",
       "    'content': 'You are a helpful assistant who answers in markdown format with a suitable emoji'},\n",
       "   {'role': 'user',\n",
       "    'content': 'List the top 5 highest mountains in the world'},\n",
       "   {'role': 'assistant',\n",
       "    'content': ' 🏔 **Top 5 Highest Mountains in the World** 🏔\\n\\n1. **Mount Everest** 🇳🇵 - 8,848.86 m (29,031.7 ft)\\n   _Located in the Mahalangur mountain range in the Himalayas._\\n\\n2. **K2** 🇵🇰 - 8,611 m (28,251 ft)\\n   _Located in the Karakoram Range in the Pakistan-China border._\\n\\n3. **Kangchenjunga** 🇳🇵 - 8,586 m (28,169 ft)\\n   _Located in the Himalayas on the border between Nepal and India._\\n\\n4. **Lhotse** 🇳🇵 - 8,516 m (27,940 ft)\\n   _Located in the Mahalangur mountain range in the Himalayas._\\n\\n5. **Makalu'}]}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=MODEL_IN_USE)\n",
    "pipe(messages_03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a1a496-118b-4109-8682-a009cf044f02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46109a6a-4241-4bf3-b750-e8133d3c0775",
   "metadata": {},
   "source": [
    "### Gated Model - Meta's Llama\n",
    "- https://huggingface.co/meta-llama/Llama-3.1-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98d7fb43-f6c0-42cd-9ff6-3169e30c9ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_IN_USE=\"meta-llama/Llama-3.1-8B\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B\", trust_remote_code=True, token=hugging_face_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e67e0d7-7fff-45bf-95b4-4b2d26e87f04",
   "metadata": {},
   "source": [
    "## Everything Wrapped in Funtion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c368c149-ffc0-46bd-912b-f598373818a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, TextStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e63ba546-cd95-4931-8e3c-c9d8967f8a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MISTRAL=\"mistralai/Mistral-7B-Instruct-v0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1775ac02-efd4-47de-bdb2-490ebed6b13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant who answers in one sentence with a suitable emoji\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who is the king of the jungle?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5195cf3f-20a2-4a92-ab44-a5732432ed97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping everything in a function - and adding Streaming and generation prompts\n",
    "\n",
    "def generate(model, messages):\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "  inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True)#.to(\"cuda\")\n",
    "  streamer = TextStreamer(tokenizer)\n",
    "  model = AutoModelForCausalLM.from_pretrained(model)\n",
    "  outputs = model.generate(inputs, max_new_tokens=80, streamer=streamer)\n",
    "  del model, inputs, tokenizer, outputs, streamer\n",
    "  gc.collect()\n",
    "  # torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1380ec0a-f017-4b78-800d-08b0f90dc1e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f71b6fd25e463bb829fc8207533ebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] You are a helpful assistant who answers in one sentence with a suitable emoji\n",
      "\n",
      "WHi is the king of the jungle? [/INST] 🐆 The king of the jungle is often considered to be the lion, but in reality, there isn't a single ruler in the jungle. "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate(MISTRAL, messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977cf293-e929-493e-ba66-53a62df0c1b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
